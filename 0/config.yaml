# Test configuration
new_test: true
test_number:

# Data paths
database_path: "E:/MARCELLO/ASSAS/ASSAS-WP2/NNmodel/HDFdatasets/dataset.h5"
scalerpath: ""
checkpoint_dir: ""

# Data processing
window_size: 10
use_embeddings: true    # This must be lowercase 'true'
is_macro: true

# Model parameters
model_name: "MLP"
embedding_dim: 512
num_layers: 10          # Numero di layer nell'encoder/decoder
layer_dim: 1024        # Dimensione di ogni layer nascosto

# Training parameters
epochs: 200
batch_size: 32
learning_rate: 0.0001
validation_split: 0.2
weight_decay: 0.00001
patience: 10
data_augmentation: false

# Preprocessing parameters
scaler_method: "MinMax"

# WindowGenerator parameters
default_encoder: false

# Neural network parameters
time_encoding_dim: 64
conv_channels: 128
n_heads: 8
dropout_rate: 0.1

# Parameter descriptions:
# new_test: Whether to create a new test directory
# test_number: Specific test number to use ONLY IF not creating new test
# database_path: Path to the directory containing HDF5 files or single HDF5 file
# scalerpath: Directory to save/load data scalers
# checkpoint_dir: Directory to save model checkpoints and embeddings
# window_size: Number of time steps in each sliding window
# use_embeddings: Whether to use dimensional reduction through embeddings
# is_macro: Use macro-scale (true) or micro-scale (false) data
# model_name: Name of the model architecture to use
# embedding_dim: Dimension of the embedded representation
# epochs: Number of training epochs
# batch_size: Number of samples per training batch
# learning_rate: Learning rate for optimizer
# validation_split: Fraction of data to use for validation
# weight_decay: L2 regularization parameter
# patience: Number of epochs without improvement before early stopping
# data_augmentation: Whether to use data augmentation during training
# scaler_method: Method to use for data scaling
# default_encoder: Use default or performance-oriented encoder architecture
# time_encoding_dim: Dimension of time encoding for performance encoder
# conv_channels: Number of channels in convolutional layers
# n_heads: Number of attention heads in transformer blocks
# dropout_rate: Dropout rate for neural networks